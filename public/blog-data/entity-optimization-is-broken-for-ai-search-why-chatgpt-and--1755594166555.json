{
  "slug": "entity-optimization-is-broken-for-ai-search-why-chatgpt-and--1755594166555",
  "title": "Entity Optimization is Broken for AI Search: Why ChatGPT and Claude Ignore 90% of Traditional Entity SEO Signals",
  "description": "Search engines taught SEOs to optimize for entities: structured data, schema markup, Knowledge Graph entries, canonicalization, consistent NAP, entity co-occurr",
  "content": "# Entity Optimization is Broken for AI Search: Why ChatGPT and Claude Ignore 90% of Traditional Entity SEO Signals\n\n## Introduction\n\nSearch engines taught SEOs to optimize for entities: structured data, schema markup, Knowledge Graph entries, canonicalization, consistent NAP, entity co-occurrence, and authority signals. For a decade, the playbook was clear — build clear entity references on pages and across the web so search engines can identify, disambiguate, and surface your brand, product, or person. But the arrival of AI-first search — where large language models (LLMs) like ChatGPT and Anthropic’s Claude generate answers and act as the interface for discovery — has exposed a major mismatch. The assumption that those classic entity signals remain the primary currency of visibility is breaking down.\n\nThis post is a technical analysis aimed at people optimizing for ranking in LLM-driven results. We'll examine why entity optimization is “broken” for AI search, what it means when I say ChatGPT and Claude ignore roughly 90% of traditional entity SEO signals (a shorthand for the practical deprecation of most classical signals), and what to do instead. I’ll weave in recent platform research: AI traffic growth, product differences between Claude and ChatGPT, and why those realities matter for entity visibility. By the end you’ll understand how modern retrieval, embeddings, grounding, and model internals re-weight signals, which signals still matter, and specific tactical shifts to regain visibility in AI search.\n\nKey data points to keep in mind while reading:\n- AI traffic surged ~527% in 2025 — users are moving to AI-first discovery rapidly.\n- ChatGPT drives 40–60% of that AI-driven volume, making its behavior critically important.\n- Claude users deliver the highest session commercial value (~$4.56 per visit), and Claude 4 introduced Opus 4 and Sonnet 4 models with notable performance gains, especially in code and reasoning tasks.\nThese platform differences shape what signals matter and which are ignored.\n\n## Understanding Entity Optimization for AI Search\n\nTo diagnose the problem we must first clarify what \"entity optimization\" meant in the classical SEO era versus what AI search platforms actually use.\n\nTraditional entity SEO: In the classical web search model, entities are nodes in a knowledge graph built from crawling and parsing the web. Classic signals include:\n- Structured data (schema.org organization, product, person markups)\n- Wikipedia / Wikidata presence\n- Consistent NAP and citations across directories\n- Outbound / inbound link graphs and co-citation patterns\n- On-page co-occurrence of entity names and attributes\n- Knowledge panel inclusion and editorial sources\n\nSearch engines built ranking and knowledge extraction pipelines that literally parse these signals to create and enrich entity records. Optimizers could influence entity rank by publishing structured markup, earning citations, and optimizing canonical signals.\n\nAI search and LLMs: LLM-based AI search is different. Models like ChatGPT and Claude are trained on massive text corpora and may be augmented with retrieval systems, dense vector stores, and specialized grounding layers. When a user asks a question, the system typically:\n1. Converts the query into vectors (embeddings).\n2. Uses a retrieval layer (information retrieval or RAG — retrieval-augmented generation) to fetch relevant chunks from an index (web snapshots, proprietary corpora, paid sources).\n3. Feeds those retrieved passages plus the model’s internal knowledge into the LLM to generate an answer, sometimes with citations.\n\nWhere classical entity signals mapped to web-scale graph structures, modern AI search relies heavily on:\n- Embedding similarity (semantic proximity),\n- Freshness and coverage of retrieval indices,\n- Passage-level relevance and answerability,\n- System-level trust heuristics (source reputation at retrieval time),\n- Model priors (what the LLM already “knows” and prefers to generate).\n\nWhy that matters: Many traditional entity signals are page-level or graph-level constructs that are poorly represented in retrieval indexes or embeddings. Structured markup, for example, is not reliably converted into dense vectors. Links and directory citations influence long-term page authority but may not change the short-list of passages retrieved for a dense embedding lookup. The effect: in practice, the retrieval model and the LLM’s internal weighting determine what surfaces — and that often sidelines the majority of traditional entity signals.\n\nWhen I say AI search “ignores 90%” of traditional signals, I am summarizing these practical observations:\n- A large fraction of schema and link-based signals do not materially affect the immediate retrieval results that feed LLM answers.\n- LLM-driven answers favor high-quality, well-written, semantically rich passages and canonical text snippets rather than structured fields.\n- The ratio of signals that materially change LLM retrieval rankings versus those that don’t is heavily skewed toward a small set of features (textual relevance, snippet density, recency), leaving many classical signals underweighted.\n\nThis is not an allegation of negligence by model builders; it’s the logical outcome of architectures that prioritize semantic retrieval and generative fluency over graph-based knowledge extraction.\n\n## Key Components and Analysis\n\nLet’s break down the core components in the AI search pipeline and analyze how traditional entity signals fare at each step.\n\n1) Training corpus and model priors\n- What happens: LLMs encode a massive amount of world knowledge into their parameter space. That knowledge is sourced from open web text, licensed data, and proprietary corpora.\n- Effect on entity signals: If a brand or entity appears frequently and in high-quality contexts within the training data, the LLM internalizes it. Training frequency often outweighs structured markup. A Wikipedia page, for example, influences a model both because it’s high-quality text and because it contains canonical language and contexts. But many local citations, directory listings, or schema fields never make it into training or are too low-value to shape the model’s priors.\n- Practical implication: Influencing model priors is effectively impossible outside of creating widely referenced, canonical text.\n\n2) Retrieval index and embeddings\n- What happens: When the system answers a query, it typically uses an embedding model to find semantically similar passages in an index. That index contains text chunks — sometimes cleansed HTML, sometimes page extracts — but rarely carries structured data with the same embedding fidelity.\n- Effect on entity signals: Schema fields and link graphs don’t translate directly into embeddings unless the pipeline explicitly converts them into textual signals. Dense retrieval favors passages that are semantically rich, well-written, and contextually relevant. Even authoritative sites without semantically relevant snippets may be skipped if their passages don’t match embeddings.\n- Practical implication: Ensuring the right passages exist — canonical, answer-focused, and semantically concentrated — is far more important than hidden structured fields.\n\n3) Answer generation and grounding\n- What happens: The LLM synthesizes retrieved passages plus its internal knowledge to generate an answer. Systems attempt grounding (cite sources) but still heavily weight the combined “signal” rather than the origin of each discrete entity attribute.\n- Effect on entity signals: A Knowledge Graph triple (brand->location->address) might be ignored if no retrieved passage encodes that triple in a way that the LLM finds useful. Conversely, a single, authoritative paragraph on a page that repeats multiple entity attributes will outperform many fragmented structured entries.\n- Practical implication: Content that reads well as a compact, authoritative passage will be preferentially used for answers and citations.\n\n4) Trust and metrics\n- What happens: AI platforms measure trust and quality with metrics like answer quality, source credibility, user feedback, and commercial relevance. Claude’s reported higher session value (~$4.56 per visit) and ChatGPT’s larger volume (40–60% of AI traffic) indicate different user mixes and measured commercial outcomes.\n- Effect on entity signals: Trust models prioritize sources with demonstrable page-level quality and real user engagement; raw structured listings don’t show up in those signals as strongly as narrative content or reputational citations.\n- Practical implication: Building trust requires visible engagement and content usefulness more than distributed structured citations.\n\n5) Frequency and freshness\n- What happens: AI search often relies on snapshotting or curated indices that emphasize recent, high-quality material. Given the 527% surge in AI traffic (2025), systems are rapidly optimizing fresh retrieval strategies.\n- Effect on entity signals: If your entity signals are fragmented across old directory pages or low-frequency mentions, they lose influence compared to recently updated content or authoritative explainers.\n- Practical implication: Maintain fresh canonical content and update passages likely to be retrieved.\n\n6) Platform-level differences: ChatGPT vs Claude\n- What happens: Claude 4 (Opus 4, Sonnet 4) introduced notable gains in agentic coding and reasoning, with reported 20–25% better performance on some coding benchmarks versus ChatGPT 4.1. ChatGPT, meanwhile, offers a broader feature set (image generation, custom GPTs, automation).\n- Effect on entity signals: Claude’s strengths in precise reasoning and optimization make it more likely to prefer high-quality, optimized, dense technical passages when offering answers. ChatGPT’s broader integrations may favor sources that can support multimodal or task-oriented queries.\n- Practical implication: The retrieval and ranking behavior across platforms will differ; optimizing for one requires understanding that platform’s retrieval biases.\n\nSynthesis: The intersection of embedding-driven retrieval and model synthesis means only a small subset of classical entity signals — canonical, semantically-rich passages, authoritative editorial content, timely updates, and strong contextual co-occurrence — materially affect what LLMs choose to surface. The rest of the traditional entity playbook carries much less weight, hence the shorthand that 90% of classical signals are effectively ignored in day-to-day LLM ranking.\n\n## Practical Applications\n\nIf you’re responsible for ranking in LLM results, here are concrete tactics to adapt entity optimization for AI search:\n\n1) Produce canonical, answer-oriented passages\n- Create concise, well-written paragraphs that explicitly state entity attributes (e.g., “Company X is headquartered in Y, founded in Z, known for A and B.”).\n- Put these passages near the top of pages and in FAQ/definition blocks; retrieval systems often chunk by paragraph.\n\n2) Optimize for embedding relevance rather than schema tokens\n- Use natural language that anticipates question phrasing. Embeddings capture semantics, so writing in the conversational tone users query matters.\n- Avoid burying facts in markup alone; schema.org helps classical search, but the same facts should appear in readable text.\n\n3) Centralize entity information into single authoritative pages\n- Consolidate scattered citations into canonical “entity pages” that act as the single source of truth. Those pages are more likely to be retrieved as dense chunks.\n\n4) Provide multiple canonical passages for different intents\n- Build short “definition” snippets, medium-length “overview” passages, and longer “technical” sections. Different queries map to different chunk sizes in retrieval; cover them all.\n\n5) Use natural citations and human-readable provenance\n- When possible, make the provenance of facts explicit in text (“according to our latest 2025 report…”) — LLM systems often prefer sourceable phrasing for grounding.\n\n6) Prioritize freshness and crawl visibility\n- Update your canonical passages frequently and ensure they are indexable by the crawling or snapshot systems used by AI platforms. Freshness matters, especially in rapidly growing AI traffic environments.\n\n7) Monitor platform-specific behavior\n- Track how ChatGPT vs Claude users interact with your content. Claude’s user base shows higher session value; if you’re targeting high commercial intent, tailor passages for precise reasoning and transaction-focused cues.\n\n8) Measure retrieval success, not just organic rankings\n- Instrument which passages are being cited or paraphrased by LLMs (via analytics, monitoring tools, or synthetic queries). Design tests that measure whether your canonical passage is retrieved for target queries.\n\n9) Invest in long-form authoritative content\n- LLMs still value depth. Long-form explainers that contain multiple tightly-written passages give the retrieval layer many opportunities to surface your content.\n\n10) Use structured data as a supporting, not primary, signal\n- Keep schema.org markup for classical search, but don’t rely on it alone. Treat it as a hygiene factor while the primary tactical focus shifts to human-readable, semantically rich passages.\n\n## Challenges and Solutions\n\nTransitioning from classical entity SEO to LLM-aware optimization brings technical and organizational challenges. Below I lay out common issues and practical solutions.\n\nChallenge 1 — Indexing opacity and retrieval black boxes\n- Problem: You can’t always tell what snapshot or corpus an AI model uses, nor how frequently it updates.\n- Solution: Focus on universality and redundancy. Publish canonical content in multiple high-quality locations (your site, major publications, public docs). Use platform APIs (where available) to submit content or request indexing, and run frequent synthetic queries to gauge presence.\n\nChallenge 2 — Schema doesn’t translate to embeddings\n- Problem: Structured fields are invisible to dense retrieval unless converted to text.\n- Solution: Echo structured fields in readable sentences. Build “data cards” that render schema content as authoritative paragraphs and include them in page intros or FAQs.\n\nChallenge 3 — Fragmented entity mentions\n- Problem: When entity data is scattered across micro-listings and directories, retrieval prefers consolidated passages.\n- Solution: Consolidate into canonical entity pages and ensure that external references link to those canonical pages with descriptive anchor text to preserve discoverability for classical crawlers and humans.\n\nChallenge 4 — Platform divergence (ChatGPT vs Claude)\n- Problem: Different LLMs and vendors have different retrieval corpora and ranking heuristics.\n- Solution: Prioritize horizontal optimizations (canonical passages, semantic relevance, freshness) that work across platforms. Additionally, create targeted assets optimized for platform strengths: precise technical passages for Claude; multimodal-ready content and procedural prompts for ChatGPT.\n\nChallenge 5 — Measuring impact\n- Problem: Conventional SEO metrics (organic clicks, rank positions) don’t directly translate to LLM visibility.\n- Solution: Expand metrics to include:\n  - Citation tracking in AI answers (where available),\n  - Synthetic query coverage tests,\n  - Conversion and engagement from AI-driven referrals,\n  - Session value metrics across platforms (benchmarks show Claude’s $4.56 per visit vs other platforms’ lower values).\n\nChallenge 6 — Model hallucination and trust\n- Problem: LLMs may hallucinate or synthesize facts if retrieval fails.\n- Solution: Make critical facts easy to retrieve: short, repeated canonical lines; explicit phrasing; and clear citations. Use content structures that reduce ambiguity (e.g., bulletized facts near page top).\n\nChallenge 7 — Resource constraints for constant content optimization\n- Problem: Constantly reauthoring content for semantic relevance demands resources.\n- Solution: Prioritize high-impact entities and queries. Use analytics and platform signals to identify pages that should be canonicalized and optimized first (product pages, high-value brand pages, conversion hubs).\n\n## Future Outlook\n\nAI search is still evolving rapidly. The 527% surge in AI traffic (2025) and the product differentials between platforms like ChatGPT and Claude mean the landscape will continue to shift. Here’s how I see the near-term future for entity optimization.\n\n1) Retrieval will become more sophisticated and transparent\n- Expect vendors to invest in retrieval explainability and indexing APIs. Vendors will likely offer tools to submit corpora, tag entities, and signal authoritative content for their retrieval indices. Early adopters who integrate will benefit.\n\n2) Schema will adapt to embeddings\n- Structured data vocabularies will expand to include semantic-friendly textual summarizations and canonical snippets designed for embedding extraction. We’ll see hybrid schema fields that map to both graph and text.\n\n3) Vendor-specific ranking layers will matter more\n- As Claude and ChatGPT differentiate, multi-platform optimization strategies will be necessary. Claude’s strengths in reasoning and Claude 4’s improved agentic performance suggest it will favor tightly optimized passages for technical/transactional queries. ChatGPT’s integrations will keep it as a broad discovery engine.\n\n4) Entity identity systems may re-emerge\n- New identity layers (publisher attestations, verified entity manifests) may be developed to help LLMs trust and disambiguate entities. Engage early with any verification programs offered by platform vendors.\n\n5) Measurement and tooling ecosystems will mature\n- Expect better monitoring tools for AI citations and LLM retrieval traces. These will make it easier to know which passages are being used and which entity attributes are getting surfaced.\n\n6) Human-centric writing regains prominence\n- The best-performing content will continue to be human-readable, semantically dense, and purpose-built for query intents. Writing that anticipates question phrasing and delivers short, verifiable answers will be rewarded.\n\n7) Commercial strategies will bifurcate\n- Given Claude’s higher session value (~$4.56 per visit in recent data) and ChatGPT’s dominant volume (40–60% of AI traffic), businesses will need to decide whether to chase high-value, precision-centered visibility or broad volume. Product, sales, and marketing teams must align with platform-specific priorities.\n\nOverall, entity optimization won’t disappear — it will be reframed. The “broken” aspect is not irreparable; it is an opportunity to re-architect how entity information is authored, stored, and surfaced for retrieval-first systems.\n\n## Conclusion\n\nCalling entity optimization “broken” for AI search is a provocative shorthand for a real technical shift: LLM-first retrieval and generation pipelines dramatically re-weight what signals matter. The practical result is that many traditional entity SEO signals — schema-only fields, dispersed citations, link graph tweaks — no longer reliably influence the snippets and passages that these systems retrieve and use to compose answers. Instead, a small set of signals matters far more: high-quality canonical passages, semantic relevance (embeddings-friendly text), freshness, and explicit provenance.\n\nThe surge in AI traffic (527% in 2025) and platform differences — ChatGPT driving 40–60% of that volume, Claude showing higher per-session commercial value and notable technical gains with Claude 4 (Opus 4, Sonnet 4) — underscore why this matters now. Technical teams optimizing for LLM visibility must shift from a schema-first checklist to a retrieval-first content strategy: consolidate entity data into readable canonical passages, optimize for semantic relevance, make provenance explicit, and monitor retrieval behavior across platforms.\n\nActionable takeaways recap:\n- Build canonical, semantically dense passages that encode entity attributes in human-readable text.\n- Echo structured data into natural language near page tops and in FAQs.\n- Prioritize freshness and multi-location authority to improve retrieval odds.\n- Run synthetic queries and monitor which passages are being cited or paraphrased by LLMs.\n- Tailor assets to platform differences: precision and technical density for Claude, multimodal and task-first assets for ChatGPT.\n\nIf you want to rank in LLM results, stop treating schema as a silver bullet. Think like a retrieval system engineer: author dense, canonical, and retrievable text, measure retrieval coverage, and iterate. The entity playbook is being rewritten — those who adapt will dominate discovery in the AI era.",
  "category": "ranking on LLM results",
  "keywords": [
    "entity SEO",
    "ChatGPT optimization",
    "AI search ranking",
    "LLM visibility"
  ],
  "tags": [
    "entity SEO",
    "ChatGPT optimization",
    "AI search ranking",
    "LLM visibility"
  ],
  "publishedAt": "2025-08-19T09:02:46.555Z",
  "updatedAt": "2025-08-19T09:02:46.556Z",
  "author": {
    "name": "AI Content Team",
    "bio": "Expert content creators powered by AI and data-driven insights"
  },
  "metrics": {
    "readingTime": 14,
    "wordCount": 2970
  }
}